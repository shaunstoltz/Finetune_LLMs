deepspeed --num_gpus=4 run_clm.py --deepspeed ds_config_stage3.json --model_name_or_path openchat/openchat_3.5 --train_file /home/shaunst/train_llama.json --validation_file /home/shaunst/eval_llama.json --do_train --do_eval --fp16 --overwrite_cache --evaluation_strategy=steps --output_dir finetuned --num_train_epochs 1  --eval_steps 1000 --tokenizer_name openchat/openchat_3.5 --load_best_model_at_end=True --report_to=wandb --preprocessing_num_workers 8 --per_device_train_batch_size 1 --logging_steps 100 --per_device_eval_batch_size 4 --save_steps 1000




deepspeed --num_gpus=4 run_clm_v2.py --deepspeed ds_config_stage3_orig.json --model_name_or_path openchat/openchat_3.5 --train_file /home/shaunst/train_llama.json --validation_file /home/shaunst/eval_llama.json --do_train --do_eval --fp16 --overwrite_cache --evaluation_strategy=steps --output_dir finetuned --num_train_epochs 1  --eval_steps 1000 --use_fast_tokenizer False --learning_rate 5e-06 --warmup_steps 10 --save_total_limit 1 --save_steps 1000 --save_strategy steps --tokenizer_name openchat/openchat_3.5 --load_best_model_at_end=True --block_size=2048 --report_to=wandb --logging_steps 100 --per_device_train_batch_size 1 --per_device_eval_batch_size 4





deepspeed --num_gpus=4 run_clm_v2.py --deepspeed ds_config_stage3_orig.json --model_name_or_path openchat/openchat_3.5 --train_file /home/shaunst/train_llama.json --validation_file /home/shaunst/eval_llama.json --do_train --do_eval --fp16 --overwrite_cache --evaluation_strategy=steps --output_dir finetuned --num_train_epochs 1  --eval_steps 1000 --use_fast_tokenizer True --learning_rate 5e-06 --warmup_steps 10 --save_total_limit 1 --save_steps 1000 --save_strategy steps --tokenizer_name openchat/openchat_3.5 --load_best_model_at_end=True --block_size=2048 --report_to=wandb --logging_steps 100 --per_device_train_batch_size 1 --per_device_eval_batch_size 4

deepspeed --num_gpus=2 run_clm_v3.py --deepspeed ds_config_stage3_orig.json --model_name_or_path HuggingFaceH4/zephyr-7b-beta --train_file train_llama.json --validation_file eval_llama.json --do_train --do_eval --bf16 --overwrite_cache --evaluation_strategy=steps --output_dir output --num_train_epochs 1  --eval_steps 100 --use_fast_tokenizer True --warmup_steps 10 --save_total_limit 2 --save_steps 100 --save_strategy steps --tokenizer_name HuggingFaceH4/zephyr-7b-beta --load_best_model_at_end=True --block_size=2048 --report_to=wandb --logging_steps 100 --per_device_train_batch_size 24 --per_device_eval_batch_size 32

python process_math_dataset.py --deepspeed ds_config_stage3_orig.json --model_name_or_path HuggingFaceH4/zephyr-7b-beta --train_file train_llama.json --validation_file eval_llama.json --do_train --do_eval --bf16 --overwrite_cache --evaluation_strategy=steps --output_dir maths --num_train_epochs 1  --eval_steps 10 --use_fast_tokenizer True --warmup_steps 10 --save_total_limit 1 --save_steps 10 --save_strategy steps --tokenizer_name HuggingFaceH4/zephyr-7b-beta --load_best_model_at_end=True --block_size=2048 --report_to=wandb --logging_steps 10 --per_device_train_batch_size 24 --per_device_eval_batch_size 32

deepspeed --num_gpus=6 run_clm_v3.py --deepspeed ds_config_stage3_orig.json --model_name_or_path 01-ai/Yi-6B --train_file train_llama.json --validation_file eval_llama.json --do_train --do_eval --bf16 --overwrite_cache --evaluation_strategy=steps --output_dir output --num_train_epochs 1  --eval_steps 100 --use_fast_tokenizer True --warmup_steps 500 --save_total_limit 1 --save_steps 100 --save_strategy steps --tokenizer_name 01-ai/Yi-6B --load_best_model_at_end=True --block_size=2048 --report_to=wandb --logging_steps 100 --per_device_train_batch_size 26 --per_device_eval_batch_size 32 --trust_remote_code


python process_original_mathset.py --deepspeed ds_config_stage3_orig.json --model_name_or_path mistralai/Mistral-7B-v0.1 --train_file train_llama.json --validation_file eval_llama.json --do_train --do_eval --fp16 --overwrite_cache --evaluation_strategy=steps --output_dir maths --num_train_epochs 1  --eval_steps 10 --use_fast_tokenizer True --warmup_steps 10 --save_total_limit 1 --save_steps 10 --save_strategy steps --tokenizer_name mistralai/Mistral-7B-v0.1 --block_size=512 --report_to=wandb --logging_steps 10 --per_device_train_batch_size 24 --per_device_eval_batch_size 32


deepspeed --num_gpus=6 run_clm_v3.py --deepspeed ds_config_stage2.json --model_name_or_path mistralai/Mistral-7B-v0.1 --train_file train_llama.json --validation_file eval_llama.json --do_train --do_eval --bf16 --overwrite_cache --evaluation_strategy=steps --output_dir outputqlora --num_train_epochs 1  --eval_steps 10000 --use_fast_tokenizer True --warmup_steps 10000 --save_total_limit 1 --save_steps 10000 --save_strategy steps --tokenizer_name mistralai/Mistral-7B-v0.1 --block_size=2048 --report_to=wandb --logging_steps 100 --per_device_train_batch_size 2 --per_device_eval_batch_size 32 --use_lora --lora_bits 4

deepspeed --num_gpus=2 run_clm_v3.py --deepspeed ds_config_stage2.json --model_name_or_path mistralai/Mistral-7B-v0.1 --train_file train_llama.json --validation_file eval_llama.json --do_train --do_eval --bf16 --overwrite_cache --evaluation_strategy=steps --output_dir outputqlora --num_train_epochs 1  --eval_steps 10000 --use_fast_tokenizer True --warmup_steps 10000 --save_total_limit 1 --save_steps 10000 --save_strategy steps --tokenizer_name mistralai/Mistral-7B-v0.1 --block_size=2048 --report_to=wandb --logging_steps 100 --per_device_train_batch_size 2 --per_device_eval_batch_size 32 --use_lora

pyhthon eval_vllm.py -m mistralai/Mistral-7B-v0.1 -id gsm8k -i ....../Finetune_LLMs/data/gsm8k/test.jsonl

